{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7151e9fd",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "リフレクションは、LLMの生成に続いてリフレクションが行われ、そのリフレクション自体が最初のLLMの出力に基づいて条件付けられた別のLLMの生成を行うデザインパターンです。例えば、コードを書くタスクが与えられた場合、最初のLLMはコードのスニペットを生成し、2番目のLLMはそのコードのスニペットに対する批判を生成します。\n",
    "\n",
    "AutoGenとエージェントの文脈では、リフレクションはエージェントのペアとして実装できます。最初のエージェントがメッセージを生成し、2番目のエージェントがそのメッセージに対する応答を生成します。2つのエージェントは、最大反復回数や2番目のエージェントの承認など、停止条件に達するまで相互作用を続けます。\n",
    "\n",
    "AutoGenエージェントを使用して、シンプルな反射デザインパターンを実装してみましょう。\n",
    "\n",
    "2つのエージェントが存在します：コーダーエージェントとレビュアーエージェントです。コーダーエージェントはコードスニペットを生成し、レビュアーエージェントはコードスニペットの批判を生成します。\n",
    "\n",
    "\n",
    "<img src=\"./img/reflection.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54ec14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U \"autogen-agentchat\"\n",
    "#!pip install \"autogen-ext[mcp,openai,azure]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c61a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging  \n",
    "from typing import Any, List  \n",
    "import os  \n",
    "from dotenv import load_dotenv  \n",
    "\n",
    "from autogen_agentchat.agents import AssistantAgent  \n",
    "from autogen_agentchat.teams import RoundRobinGroupChat  # keeps implementation simple & familiar  \n",
    "from autogen_agentchat.conditions import TextMessageTermination, MaxMessageTermination, TextMentionTermination, TimeoutTermination\n",
    "\n",
    "from autogen_core import CancellationToken  \n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient  \n",
    "from autogen_ext.tools.mcp import SseServerParams, mcp_server_tools  \n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df7d431",
   "metadata": {},
   "source": [
    "## OpenTelemetry によるトレーサーのセット\n",
    "マルチエージェントのデバッグには OpenTelemetry によるトレーサーを利用すると便利。`OpenAIInstrumentor` を使用して OpenAI コールをキャプチャできます。ここではトレース UI として [Jaeger](https://www.jaegertracing.io/download/) を使用しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d823abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp opentelemetry-instrumentation-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2087c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.instrumentation.openai import OpenAIInstrumentor\n",
    "\n",
    "service_name = \"autogen\"\n",
    "\n",
    "# OTLPエクスポーターの設定 (gRPC経由で送信)\n",
    "otlp_exporter = OTLPSpanExporter(\n",
    "    endpoint=\"http://localhost:4317\",  # JaegerのgRPCエンドポイント\n",
    ")\n",
    "tracer_provider = TracerProvider(resource=Resource({\"service.name\": service_name}))\n",
    "    \n",
    "# トレーサープロバイダーの設定\n",
    "trace.set_tracer_provider(tracer_provider)\n",
    "\n",
    "# バッチスパンプロセッサーを設定\n",
    "span_processor = BatchSpanProcessor(otlp_exporter)\n",
    "tracer_provider.add_span_processor(span_processor)\n",
    "\n",
    "# トレーサーを取得\n",
    "tracer = tracer_provider.get_tracer(service_name)\n",
    "\n",
    "OpenAIInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad0654",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_server_uri = os.getenv(\"MCP_SERVER_URI\")\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_model = os.getenv(\"AZURE_OPENAI_MODEL\")\n",
    "azure_deployment = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "openai_model_name = os.getenv(\"OPENAI_MODEL_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad69a116",
   "metadata": {},
   "source": [
    "## Tools の定義(MCP over SSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8de4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "server_params = SseServerParams(  \n",
    "    url=mcp_server_uri,  \n",
    "    headers={\"Content-Type\": \"application/json\"},  \n",
    "    timeout=30  \n",
    ")  \n",
    "\n",
    "# Fetch tools (async)  \n",
    "tools = await mcp_server_tools(server_params)  \n",
    "print(f\"Number of Tools: {len(tools)}\")\n",
    "# Set up the OpenAI/Azure model client  \n",
    "model_client = AzureOpenAIChatCompletionClient(  \n",
    "    api_key=azure_openai_key,  \n",
    "    azure_endpoint=azure_openai_endpoint,  \n",
    "    api_version=api_version,  \n",
    "    azure_deployment=azure_deployment,  \n",
    "    model=openai_model_name,  \n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8be2810",
   "metadata": {},
   "source": [
    "## エージェント定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b15d14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. -----------------  Agent Definitions -----------------  \n",
    "primary_agent = AssistantAgent(  \n",
    "    name=\"primary\",  \n",
    "    model_client=model_client,  \n",
    "    tools=tools,  \n",
    "    description=\"役立つアシスタント。複数のツールを使用して情報を検索し、質問に回答する\",\n",
    "    system_message=(  \n",
    "\"\"\"\n",
    "あなたは役立つアシスタントです。複数のツールを使用して情報を検索し、質問に回答することができます。\n",
    "利用可能なツールを確認し、必要に応じて使用してください。ユーザーが不明な点がある場合は、確認のための質問をすることもできます。\n",
    "\"\"\"\n",
    ")\n",
    ")  \n",
    "\n",
    "critic_agent = AssistantAgent(  \n",
    "    name=\"critic\",  \n",
    "    model_client=model_client,  \n",
    "    description=\"建設的なフィードバックを提供するデータアナリスト。主に他のエージェントの出力を評価し、改善点を提案する役割を担う。\",\n",
    "    tools=tools,  \n",
    "    system_message=(\n",
    "\"\"\"\n",
    "プロのデータアナリストとして建設的なフィードバックを提供してください。フィードバックが反映された場合は 'APPROVE' と回答してください。\n",
    "\"\"\"\n",
    "    ),  \n",
    ")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6327dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. -----------------  Assemble Team -----------------  \n",
    "participants: List[AssistantAgent] = [  \n",
    "    primary_agent,  \n",
    "    critic_agent\n",
    "]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed51905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define termination condition\n",
    "max_msg_termination = MaxMessageTermination(max_messages=15)\n",
    "termination_condition = TextMessageTermination(\"primary\") \n",
    "time_terminarion = TimeoutTermination(120)\n",
    "combined_termination = max_msg_termination | termination_condition | time_terminarion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d933b648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. -----------------  Create Team Agent -----------------\n",
    "team_agent = RoundRobinGroupChat(\n",
    "    participants=participants,\n",
    "    termination_condition=combined_termination,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8d5a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tracer.start_as_current_span(\"Reflection\") as rollspan: # ルートスパンを作成\n",
    "    task = \"ユーザーID:123 の出荷状況を確認してください。\"\n",
    "\n",
    "    await Console(team_agent.run_stream(task=task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581fb0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tracer.start_as_current_span(\"Reflection\") as rollspan: # ルートスパンを作成\n",
    "    #task = \"ユーザーID:123 の出荷状況を確認してください。あと頼んだのって何の商品だっけ\"\n",
    "    task = \"商品ID:339の商品詳細を教えて\"\n",
    "    await Console(team_agent.run_stream(task=task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9079c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tracer.start_as_current_span(\"Reflection\") as rollspan: # ルートスパンを作成\n",
    "#task = \"ユーザーID:123 の出荷状況を確認してください。あと頼んだのって何の商品だっけ\"\n",
    "    task = \"SNS分析を行い、日付ごとのツイート数を集計してください\"\n",
    "    await Console(team_agent.run_stream(task=task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645340b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tracer.start_as_current_span(\"Reflection\") as rollspan: # ルートスパンを作成\n",
    "    task = \"2024年9月~10月の受注数を集計し、最も受注数が多いかった日付を教えて\"\n",
    "    await Console(team_agent.run_stream(task=task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66d95a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tracer.start_as_current_span(\"Reflection\") as rollspan: # ルートスパンを作成\n",
    "    task = \"2024年9月~10月の受注数を集計し、さらにSNS分析を行い、日付ごとのツイート数を集計した結果何かわかることはあるか？\"\n",
    "    await Console(team_agent.run_stream(task=task))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen059",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
