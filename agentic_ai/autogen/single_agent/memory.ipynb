{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "751acbb1",
   "metadata": {},
   "source": [
    "# Memory and RAG\n",
    "\n",
    "特定のステップ直前にエージェントのコンテキストにインテリジェントに追加できる有用な事実のストアを維持することが価値ある複数のユースケースが存在します。ここでの典型的なユースケースは、データベースから関連情報を取得し、その情報をエージェントのコンテキストに追加するRAGパターンです。\n",
    "\n",
    "AgentChat は、この機能を提供するために拡張可能な Memory プロトコルを提供しています。主要なメソッドは、query、update_context、add、clear、および close です。  \n",
    "\n",
    "- add： メモリストアに新しいエントリを追加します  \n",
    "- query： メモリストアから関連する情報を取得します  \n",
    "- update_context： 取得した情報をエージェントの内部モデルコンテキストに追加して変更します（AssistantAgentクラスで使用されます）  \n",
    "- clear： メモリストアからすべてのエントリを削除します  \n",
    "- close： メモリストアが使用しているリソースを解放します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a58df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "from dotenv import load_dotenv  \n",
    "  \n",
    "from autogen_agentchat.agents import AssistantAgent  \n",
    "from autogen_agentchat.teams import RoundRobinGroupChat  \n",
    "from autogen_agentchat.conditions import TextMessageTermination  \n",
    "from autogen_core import CancellationToken  \n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient  \n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.tools.mcp import SseServerParams, mcp_server_tools  \n",
    "\n",
    "from autogen_core.memory import ListMemory, MemoryContent, MemoryMimeType\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0785d1e0",
   "metadata": {},
   "source": [
    "## OpenTelemetry によるトレーサーのセット\n",
    "マルチエージェントのデバッグには OpenTelemetry によるトレーサーを利用すると便利。`OpenAIInstrumentor` を使用して OpenAI コールをキャプチャできます。ここではトレース UI として [Jaeger](https://www.jaegertracing.io/download/) を使用しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d668f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.instrumentation.openai import OpenAIInstrumentor\n",
    "\n",
    "service_name = \"autogen\"\n",
    "\n",
    "# OTLPエクスポーターの設定 (gRPC経由で送信)\n",
    "otlp_exporter = OTLPSpanExporter(\n",
    "    endpoint=\"http://localhost:4317\",  # JaegerのgRPCエンドポイント\n",
    ")\n",
    "tracer_provider = TracerProvider(resource=Resource({\"service.name\": service_name}))\n",
    "    \n",
    "# トレーサープロバイダーの設定\n",
    "trace.set_tracer_provider(tracer_provider)\n",
    "\n",
    "# バッチスパンプロセッサーを設定\n",
    "span_processor = BatchSpanProcessor(otlp_exporter)\n",
    "tracer_provider.add_span_processor(span_processor)\n",
    "\n",
    "# トレーサーを取得\n",
    "tracer = tracer_provider.get_tracer(service_name)\n",
    "\n",
    "OpenAIInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b3ef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_server_uri = os.getenv(\"MCP_SERVER_URI\")\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_model = os.getenv(\"AZURE_OPENAI_MODEL\")\n",
    "azure_deployment = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "openai_model_name = os.getenv(\"OPENAI_MODEL_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f87740e",
   "metadata": {},
   "source": [
    "## Tools の定義(MCP over SSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44297f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up the OpenAI/Azure model client  \n",
    "model_client = AzureOpenAIChatCompletionClient(  \n",
    "    api_key=azure_openai_key,  \n",
    "    azure_endpoint=azure_openai_endpoint,  \n",
    "    api_version=api_version,  \n",
    "    azure_deployment=azure_deployment,  \n",
    "    model=openai_model_name,  \n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be0700d",
   "metadata": {},
   "source": [
    "## Memory 実装\n",
    "記憶を時系列順に保持し、最も最近の記憶をモデルのコンテキストに追加するシンプルなリストベースの記憶実装です。この実装は、シンプルで予測可能な設計となっており、理解やデバッグが容易です。以下の例では、ListMemoryを使用してユーザーの設定を保持するメモリバンクを実装し、エージェントの応答に一貫したコンテキストを提供する方法示します。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7babf1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize user memory\n",
    "user_memory = ListMemory()\n",
    "\n",
    "# Add user preferences to memory\n",
    "await user_memory.add(MemoryContent(content=\"喋る時は関西弁を好む\", mime_type=MemoryMimeType.TEXT))\n",
    "\n",
    "await user_memory.add(MemoryContent(content=\"批判的な観点から指摘されるのを嫌う\", mime_type=MemoryMimeType.TEXT))\n",
    "\n",
    "await user_memory.add(MemoryContent(content=\"出力形式は箇条書きで3つにまとめないと満足しない\", mime_type=MemoryMimeType.TEXT))\n",
    "\n",
    "user_memory.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379005c4",
   "metadata": {},
   "source": [
    "## エージェント定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76b07d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the assistant agent  \n",
    "agent = AssistantAgent(  \n",
    "    name=\"ai_assistant\",  \n",
    "    model_client=model_client,  \n",
    "    system_message=(  \n",
    "        \"あなたは役立つアシスタントです。複数のツールを使用して情報を検索し、質問に回答することができます。\"  \n",
    "        \"利用可能なツールを確認し、必要に応じて使用してください。ユーザーが不明点がある場合は、確認のための質問をすることもできます。\"\n",
    "    ),\n",
    "    memory=[user_memory]\n",
    ")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2daf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the agent with a task.\n",
    "stream = agent.run_stream(task=\"今後のエンタープライズAIエージェント市場について君はどう考える？\",)\n",
    "await Console(stream)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306ffb10",
   "metadata": {},
   "source": [
    "# Task-Centric Memory\n",
    "AutoGen で実験的・研究的機能として実装中の Task-Centric Memory は、AI エージェントが「ユーザーから与えられた助言・ルール・ヒント・デモ・失敗から得た知見」などを“記憶”し、将来の類似タスクや会話でそれらを自動的に活用できるようにするためのメモリー機構です。\n",
    "\n",
    "タスクとは、アプリがエージェントに与えるテキスト指示のことです。インサイトとは、エージェントがそのようなタスクを実行するのに役立つ可能性のあるテキスト（ヒント、アドバイス、デモンストレーション、計画など）のことです。\n",
    "\n",
    "## Task-Centric Memory の特徴\n",
    "- 「タスク解決・学習」に特化したメモリ設計\n",
    "- ユーザーのアドバイス・デモ・エージェント自身の失敗から得た インサイト（知見） を「タスク」と紐付けて保存\n",
    "- 類似タスクへの応用・一般化・自己学習を重視\n",
    "- MemoryController や Teachability など、学習ループや知見抽出・再利用のための高度なロジックを持つ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365e2767",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"autogen-ext[task-centric-memory]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7730cf33",
   "metadata": {},
   "source": [
    "## 1. 直接的な記憶の保存と検索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f600e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.models.openai import OpenAIChatCompletionClient,AzureOpenAIChatCompletionClient\n",
    "from autogen_ext.experimental.task_centric_memory import MemoryController\n",
    "\n",
    "memory_controller = MemoryController(reset=True, client=model_client)\n",
    "\n",
    "# 1. タスクとインサイト（ヒント/教え）を登録\n",
    "await memory_controller.add_memo(task=\"友人Aさんに子供が生まれたので出産祝いを贈りたい\", insight=\"A様のご友人は2人目のお子様とのことで、既に基本的な育児グッズは揃っている可能性がある\")\n",
    "\n",
    "# 2. 新しいタスクに対して関連インサイトを検索\n",
    "new_task = \"友人Aに出産祝いを贈りたいんですが、何がいいですか？\"\n",
    "memos = await memory_controller.retrieve_relevant_memos(task=new_task)\n",
    "\n",
    "print(f\"Task: {new_task}\")\n",
    "print(\"Retrieved insights:\")\n",
    "for memo in memos:\n",
    "    print(\"-\", memo.insight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80119d8",
   "metadata": {},
   "source": [
    "### 記憶の永続化\n",
    "`MemoryController(reset=False)` とすることで `MemoryBank(pickle, ChromaDB)` に永続化されます。入力（タスクやトピック）に対して、類似するメモ（インサイト）をベクトル検索で高速に取得できるようになります。デフォルト実装では抽出したタスクを ChromaDB に保存しベクトル検索、タスクに対応するインサイトを pickle からロードするという実装になっています。\n",
    "\n",
    "タスク自体をベクトル化し、類似検索することで、過去の“教え”や“インサイト”を“広範な未経験タスク”にも応用できるという仕組みになっています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cdff5a",
   "metadata": {},
   "source": [
    "## 2. ユーザーのアドバイスや修正から学ぶ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36bd73a",
   "metadata": {},
   "source": [
    "では、このタスクとインサイトのセットを会話から自動的に抽出するにはどのように実装すれば良いのでしょうか。\n",
    "\n",
    "AssistantAgent に Teachability クラスをセットすることでユーザーが入力した発言の中から「教え（助言・ルール）」が自動で抽出・保存されるようになっています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c56d0ef",
   "metadata": {},
   "source": [
    "例えば、以下のような発言をしてみましょう。\n",
    "- `要約タスクは必ず3行でお願いします`\n",
    "- `要約タスクは関西弁でしゃべる必要があります`\n",
    "- `あなたは何を記憶していますか？`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff5f84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.experimental.task_centric_memory.utils import Teachability\n",
    "\n",
    "# Create an instance of Task-Centric Memory, passing minimal parameters for this simple example\n",
    "memory_controller = MemoryController(reset=False, client=model_client)\n",
    "\n",
    "# Wrap the memory controller in a Teachability instance\n",
    "teachability = Teachability(memory_controller=memory_controller)\n",
    "\n",
    "memory_controller.reset_memory()\n",
    "\n",
    "# Create an AssistantAgent, and attach teachability as its memory\n",
    "assistant_agent = AssistantAgent(\n",
    "    name=\"teachable_agent\",\n",
    "    system_message = \"あなたは過去の会話からユーザーの指示を記憶する特別な能力を持つ、役立つAIアシスタントです。\",\n",
    "    model_client=model_client,\n",
    "    memory=[teachability],\n",
    ")\n",
    "\n",
    "# Enter a loop to chat with the teachable agent\n",
    "print(\"現在、学習可能なエージェントとチャット中です。最初のメッセージを入力してください。終了するには「exit」または「quit」と入力してください。\")\n",
    "while True:\n",
    "    user_input = input(\"\\nYou: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    await Console(assistant_agent.run_stream(task=user_input))\n",
    "\n",
    "# Close the connection to the client\n",
    "await model_client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff49ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# ファイルをバイナリ読み込みモードで開く\n",
    "with open(\"./memory_bank/default/string_map/uid_text_dict.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023edccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイルをバイナリ読み込みモードで開く\n",
    "with open(\"./memory_bank/default/uid_memo_dict.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a7bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# 1. データベースファイルに接続\n",
    "conn = sqlite3.connect(\"./memory_bank/default/string_map/chroma.sqlite3\")\n",
    "\n",
    "# 2. カーソルを取得\n",
    "cur = conn.cursor()\n",
    "\n",
    "# 3. 特定テーブルの中身をすべて表示\n",
    "cur.execute(\"SELECT seq_id, metadata, encoding FROM embeddings_queue;\")  # ← mytable を参照したいテーブル名に変更\n",
    "rows = cur.fetchall()\n",
    "\n",
    "# 行データを表示\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# 6. 終了処理\n",
    "cur.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
